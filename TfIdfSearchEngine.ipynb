{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34ef1379",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Create functions\n",
    "- refactor code with functions\n",
    "- Apply to python file\n",
    "- Create command line tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0feb3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, math\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from stemming.porter2 import stem\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7da02e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(txt:str) -> list:\n",
    "    \"\"\"Receives a string of text, removes the punctuation, turns text into \n",
    "    lowercase and performs stemming. Returns the input string as a list of \n",
    "    tokens\"\"\"\n",
    "    txt = txt.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    txt = list(map(stem, txt.split()))\n",
    "    return txt\n",
    "\n",
    "def tf_func (text_list: list) -> dict:\n",
    "    \"\"\"Returns a dictionary containing the word the its term frequency\"\"\"\n",
    "    return_dict = {}\n",
    "    x = Counter(text_list)\n",
    "    max_occurence = x.most_common(1)[0][1]\n",
    "    x = dict(sorted(x.items()))\n",
    "    \n",
    "    for word, count in x.items():\n",
    "        tf = count/max_occurence\n",
    "        return_dict.update({word:tf})\n",
    "    return return_dict\n",
    "\n",
    "def idf_func(vocab:dict, corpus_len:int) -> dict:\n",
    "    \"\"\"Function to calculate IDF over vocab dict. The vocab dict is a dict\n",
    "    that contains the doc IDs as keys and the text as a list of tokens in its\n",
    "    values\"\"\"\n",
    "    idf_dict = {}\n",
    "    for term,count in vocab.items():\n",
    "        idf = math.log(corpus_len/count)\n",
    "        idf_dict[term] = idf\n",
    "    return idf_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "53f287b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collectionName = \"nytsmall\"\n",
    "#collectionName = \"nyt199501\"\n",
    "\n",
    "tree = etree.parse( collectionName + \".xml\")\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ac74b874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5657ae924c754c5a8ff24f0a74115817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = {}\n",
    "corpus = {}\n",
    "\n",
    "\n",
    "for doc in tqdm(root.iter('DOC')):\n",
    "    doc_txt = \"\"\n",
    "    for element in doc.iter('HEADLINE', \"P\", \"TEXT\"):\n",
    "        doc_txt += element.text\n",
    "\n",
    "    # Pre Process\n",
    "    doc_txt = pre_process(doc_txt)\n",
    "    \n",
    "    # Creating vocab dict\n",
    "    for word in set(doc_txt):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "    # Creating corpus dict\n",
    "    corpus[doc.attrib['id']] = doc_txt\n",
    "vocab = dict(sorted(vocab.items()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bbc88bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting corpus dict\n",
    "corpus = dict(sorted(corpus.items()))\n",
    "# n_docs = len(corpus)\n",
    "\n",
    "# IDF\n",
    "idf_dict = idf_func(vocab,len(corpus))\n",
    "\n",
    "with open (collectionName + '.idf', 'w') as file:\n",
    "    for term, idf in idf_dict.items():\n",
    "        file.write('{}\\t{}\\n'.format(term,idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "97e2c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "tf_dict = {}\n",
    "for doc_id, text in corpus.items():\n",
    "    tf_dict[doc_id] = tf_func(text)\n",
    "\n",
    "with open(collectionName+\".tf\",\"w\") as file:\n",
    "    for doc_id, w_tf_pair in tf_dict.items():\n",
    "        for word, tf in w_tf_pair.items():\n",
    "            file.write('{}\\t{}\\t{}\\n'.format(doc_id,word,tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3046354",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "\n",
    "word_to_ix = {key:value for key,value in zip(vocab.keys(),range(len(vocab)))}\n",
    "\n",
    "for doc_id,text in corpus.items():\n",
    "    tf_idf[doc_id] = np.zeros(len(vocab))   \n",
    "    \n",
    "    for term in text:\n",
    "        \n",
    "        tf_idf[doc_id][word_to_ix[term]] = tf_dict[doc_id][term] *idf_dict[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "db25e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['hurricane', 'philadelphia']\n",
    "\n",
    "\n",
    "# pre process query:\n",
    "query_str = \" \".join(query)\n",
    "query = pre_process(query_str)\n",
    "\n",
    "# Query TF:\n",
    "query_tf = tf_func(query)\n",
    "\n",
    "# Query TF-IDF\n",
    "query_tf_idf = np.zeros(len(vocab))\n",
    "\n",
    "for term in query:\n",
    "    q = query_tf[term]\n",
    "    i = idf_dict[term]\n",
    "    query_tf_idf[word_to_ix[term]] = query_tf[term] * idf_dict[term]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e5201e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF similarity search\n",
    "\n",
    "\n",
    "sim_dict = {}\n",
    "for doc_id, value in tf_idf.items():\n",
    "    top = np.dot(query_tf_idf, value)\n",
    "    bottom = np.linalg.norm(query_tf_idf) * np.linalg.norm(value)\n",
    "    sim = top/bottom\n",
    "    if sim == 0:\n",
    "        continue\n",
    "    else:\n",
    "        sim_dict.update({doc_id:top/bottom})\n",
    "\n",
    "sim_dict = sorted(sim_dict.items(),reverse=True, key=lambda item: item[1])\n",
    "print(type(sim_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7533e1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NYT_ENG_19950101.0001', 0.1523105228405513),\n",
       " ('NYT_ENG_19950101.0056', 0.029142469066059024),\n",
       " ('NYT_ENG_19950101.0022', 0.028561929187225343),\n",
       " ('NYT_ENG_19950101.0017', 0.016747860911235552)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcd248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
